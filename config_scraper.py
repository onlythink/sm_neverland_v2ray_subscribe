import requests
from bs4 import BeautifulSoup
import re

MAX_LINKS_PER_CHANNEL = 20
CHANNEL_FILE = "channels.txt"
OUTPUT_FILE = "v2ray_configs.txt"

patterns = [
    r'vmess://[a-zA-Z0-9+/=._-]+',
    r'vless://[a-zA-Z0-9@:/?=&#._-]+',
    r'trojan://[a-zA-Z0-9@:/?=&#._-]+',
    r'shadowsocks://[a-zA-Z0-9@:/?=&#._-]+',
]

def extract_links_from_channel(url):
    print(f"ğŸ” Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø²: {url}")
    res = requests.get(url)
    if res.status_code != 200:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† {url}")
        return []
    soup = BeautifulSoup(res.text, "html.parser")
    messages = soup.find_all("div", class_="tgme_widget_message_text")
    links = []
    for msg in messages:
        text = msg.get_text()
        for pattern in patterns:
            found = re.findall(pattern, text)
            for link in found:
                if link not in links:
                    links.append(link)
        if len(links) >= MAX_LINKS_PER_CHANNEL:
            break
    return links[:MAX_LINKS_PER_CHANNEL]

def main():
    all_links = []
    with open(CHANNEL_FILE, "r") as f:
        channels = f.read().splitlines()
    for ch in channels:
        links = extract_links_from_channel(ch)
        all_links.extend(links)
    all_links = list(dict.fromkeys(all_links))
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        for link in all_links:
            f.write(link.strip() + "\n")
    print(f"\nâœ… Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ú©Ø§Ù…Ù„ Ø´Ø¯. {len(all_links)} Ú©Ø§Ù†ÙÛŒÚ¯ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯ Ø¯Ø±: {OUTPUT_FILE}")

if __name__ == "__main__":
    main()
